# Active Context: SpendSense

## Current Work Focus
**Status**: Backend deployed to Railway (moved from AWS Lambda for ease of deployment)

## Recent Changes
- ✅ **Deployment Migration: AWS Lambda → Railway**
  - Moved backend deployment from AWS Lambda to Railway for simplified deployment
  - Removed Lambda-specific code (Mangum adapter, Lambda environment detection)
  - Updated deployment documentation to reflect Railway setup
  - Auto-seeding implementation:
    - `backend/app/utils/seed_data.py` seeds database from JSON files on startup if empty
    - Works in both local dev and Railway production environments
    - Handles data directory location automatically
  - Documentation:
    - Updated `README.md` with Railway deployment instructions
    - Removed Lambda-specific documentation files
    - Updated PRD.md to reflect Railway deployment
- ✅ **PR #31 Complete: Frontend - Metrics Display in Operator Dashboard (all 30 tasks finished)**
  - Created `frontend/src/components/MetricsDisplay.jsx`:
    - Displays 4 evaluation metrics (coverage, explainability, latency, auditability)
    - Progress bars for percentage metrics with color coding (>95% green, 80-95% yellow, <80% red)
    - Badges showing status (Excellent/Good/Needs Improvement)
    - Latency display with average and P95 values
  - Updated `frontend/src/pages/OperatorDashboard.jsx`:
    - Added "Run Evaluation" button with loading state and success/error handling
    - Integrated MetricsDisplay component to show evaluation metrics
    - Added Parquet Exports section: Lists S3 exports with file name, size, date, and download buttons
    - Added Evaluation History section: Shows last 5 evaluation runs with key metrics and "View Details" button
    - Auto-refresh functionality: Automatically refreshes metrics every 60 seconds (toggleable checkbox)
    - Manual refresh button for on-demand updates
    - Last updated timestamp display with relative time formatting
  - Added evaluation API functions to `frontend/src/lib/apiService.js`:
    - `runEvaluation()` - Runs evaluation and returns metrics
    - `getLatestEvaluation()` - Gets most recent evaluation metrics
    - `getEvaluationHistory()` - Gets evaluation history with limit parameter
    - `getLatestExports()` - Gets latest S3 parquet exports with pre-signed URLs
  - All 30 tasks completed (dashboard updates, metrics display, download links, history, auto-refresh)
- ✅ **PR #30 Complete: Evaluation API Endpoint (all 34 tasks finished)**
  - Created `backend/app/routers/evaluation.py` with 4 endpoints:
    - POST `/evaluate/` - Runs evaluation, computes all metrics, saves to DB, exports to S3, returns metrics with download URLs
    - GET `/evaluate/latest` - Returns most recent evaluation metrics
    - GET `/evaluate/history` - Returns evaluation history with optional limit parameter
    - GET `/evaluate/exports/latest` - Lists latest S3 exports with pre-signed URLs
  - Added `EvaluationRequest` schema to `backend/app/schemas.py` for optional run_id in request body
  - Created `run_evaluation_with_db()` function that adapts evaluation script functions to work with FastAPI dependency injection
  - Router registered in `backend/app/main.py`
  - All endpoints tested and verified via Swagger UI
  - All 34 tasks completed (router creation, endpoints, router registration, testing)
- ✅ **PR #29 Complete: Parquet Export & S3 Integration (all 45 tasks finished)**
  - Dependencies added: boto3==1.29.7, pyarrow==14.0.1 to requirements.txt
  - S3 bucket created: `spendsense-analytics-goico` in us-east-2 region
  - Parquet export functions:
    - `export_user_features_to_parquet()`: Exports user features for 30d/180d windows
    - `export_evaluation_results_to_parquet()`: Exports evaluation metrics
  - S3 upload function: `upload_to_s3()` with error handling for missing credentials
  - Evaluation report generation: `generate_evaluation_report()` creates JSON with metrics and S3 URLs
  - Integration: Updated `run_evaluation()` and `main()` to export and upload all 3 files (30d features, 180d features, evaluation results)
  - File organization: `features/` prefix for user features, `eval/` prefix for evaluation results
  - Pre-signed URLs: 7-day expiry for secure downloads
  - Testing: All files verified, S3 uploads working, parquet files readable with pandas
  - All 45 tasks completed (S3 setup, dependencies, export functions, upload functions, integration, testing)
- ✅ **PR #44 Complete: Product Management API (all 57 tasks finished)**
  - Created `backend/app/routers/products.py` with full CRUD endpoints:
    - GET `/products/` - List products with filtering (active_only, category, persona_type) and pagination (skip, limit)
    - GET `/products/{product_id}` - Get single product by ID
    - POST `/products/` - Create new product (generates prod_XXX product_id, converts lists to JSON)
    - PUT `/products/{product_id}` - Update existing product (updates all fields, updates updated_at timestamp)
    - DELETE `/products/{product_id}` - Deactivate product (soft delete: sets active=False)
  - JSON field parsing: Helper function `_parse_json_fields()` parses persona_targets and benefits from JSON strings
  - Error handling: 404 for not found, 500 for server errors, comprehensive logging
  - Router registered in `backend/app/main.py` with tag="products"
  - OpenAPI documentation: All endpoints documented with FastAPI auto-generated docs
  - All 57 tasks completed (router creation, CRUD endpoints, router registration, testing)
- ✅ **PR #43 Complete: Frontend Product Recommendation Display (all 75 tasks finished)**
  - Created separate card components for product recommendations to avoid layout issues:
    - `ProductRecommendationCard.jsx` for Approval Queue: Displays product_name instead of content, keeps all approval workflow buttons
    - `UserProductRecommendationCard.jsx` for User Dashboard: Taller card with full product details (benefits, APY, partner link, disclosure)
    - Both cards use separate components instead of conditional rendering to prevent layout conflicts
  - Updated UserDashboard:
    - Separates educational and product recommendations into distinct sections
    - Educational recommendations in existing grid layout
    - Product recommendations in separate section with side-by-side 2-column grid
    - Section headers: "Educational Recommendations" and "Product Recommendations"
  - Updated OperatorUserDetail:
    - Detects content_type for each recommendation
    - For products: Shows product_name and short_description initially
    - "Show more" expands to show full product details (benefits list, APY, partner info, disclosure)
    - Handles structured data (benefits list) with proper JSON parsing and error handling
    - Persona badge positioned on its own line to avoid alignment issues
  - Updated OperatorApprovalQueue:
    - Conditionally renders ProductRecommendationCard for partner_offer recommendations
    - Conditionally renders RecommendationCard for education recommendations
    - Approval workflow works for both types
  - Card styling improvements:
    - Persona badge positioned in top right corner with absolute positioning
    - All other content centered in card headers
    - Increased padding (pb-6 pt-6) for better spacing
    - Product cards have light blue/purple gradient background to distinguish from education
  - All 75 tasks completed (card components, dashboard updates, styling, testing)
- ✅ **PR #42 Complete: Hybrid Recommendation Engine + Product Data Normalization (all 119 tasks finished)**
  - Hybrid recommendation engine implemented:
    - Updated Pydantic schemas: Added `ProductOfferBase`, `ProductOfferResponse`, updated `RecommendationResponse` with optional product fields and content_type
    - Created `generate_combined_recommendations()` function in `recommendation_engine.py`:
      - Generates product recommendations first (via `match_products()` and eligibility filtering)
      - Educational recommendations limited to 3 *only if* product recommendations are found; otherwise all educational recommendations included
      - Combines educational and product recommendations into single list (3-5 total)
    - Updated database models: Added `product_id` column to `Recommendation` model, made `content` nullable
    - Updated API endpoints: POST `/recommendations/generate/{user_id}` uses combined logic, GET `/recommendations/{user_id}` includes product data
    - Test script created: `scripts/test_hybrid_recommendations.py` with comprehensive coverage
  - Product catalog data normalization:
    - Regenerated product catalog with ~150 products (5 batches of ~30 products each)
    - Modified `scripts/generate_product_catalog.py`:
      - Reverted prompt changes (removed explicit eligibility rules from LLM prompt)
      - Implemented batch generation to overcome OpenAI output token limits
      - Applied deterministic eligibility rules post-LLM generation:
        - `requires_no_existing_savings = TRUE` only for HYSA products
        - `requires_no_existing_investment = TRUE` only for investment/robo_advisor products
        - `min_income`, `max_credit_utilization`, `min_credit_score` set deterministically based on category with random values within ranges
    - Database schema updates:
      - Added "loan" as valid `product_type` (updated models, schemas, and type_mapping)
      - Made `recommendations.content` nullable (for partner_offer recommendations)
    - Migration consolidation:
      - Added `apply_migrations()` function to `backend/app/database.py` for automatic migration on startup
      - Removed temporary migration scripts (migrate_add_product_id.py, migrate_add_loan_product_type.py, migrate_make_content_nullable.py)
      - Removed `refresh_product_catalog.py` script (one-time use complete)
    - All 119 tasks completed (hybrid engine + data normalization)
- ✅ **PR #41 Complete: Enhanced Guardrails - Product Eligibility (all 66 tasks finished)**
  - Updated `backend/app/services/guardrails.py` with product eligibility checking:
    - Added `check_product_eligibility()` function that checks:
      - Income requirements (min_income)
      - Credit utilization requirements (max_credit_utilization)
      - Existing savings requirement (requires_no_existing_savings)
      - Existing investment requirement (requires_no_existing_investment)
      - Category-specific rules (balance_transfer requires utilization >= 0.3)
    - Returns tuple of (is_eligible: bool, reason: str) with comprehensive logging
    - Added `filter_eligible_products()` function for batch filtering:
      - Accepts list of product matches with scores
      - Runs eligibility check for each product
      - Returns filtered list of eligible products only
      - Logs reasons for filtered products
  - Updated `backend/app/services/product_matcher.py`:
    - Integrated eligibility filtering into `match_products()` function
    - Products are now filtered by eligibility after scoring and sorting
    - Only eligible products are returned to users
  - Created test script (`scripts/test_product_eligibility.py`):
    - Tests income requirement eligibility
    - Tests credit utilization requirement eligibility
    - Tests existing account requirements (savings and investment)
    - Tests category-specific rules (balance transfer)
    - Tests full flow: match + filter
    - Comprehensive test coverage for all eligibility scenarios
  - All 66 tasks completed (eligibility checker, batch filtering, integration, testing)
- ✅ **PR #40 Complete: Product Matching Service (all 91 tasks finished)**
  - Created `backend/app/services/product_matcher.py` service file
  - Helper functions: `get_account_types()`, `has_hysa()`, `has_investment_account()`
  - Relevance scoring: `calculate_relevance_score()` function with category-specific rules:
    - Balance transfer: High utilization (>0.5: +0.3, >0.7: +0.2 bonus), interest charges (+0.2)
    - HYSA: Savings activity (net_inflow > 0 and emergency_fund < 3 months: +0.4), growth rate (+0.2), penalizes existing HYSA (-0.5)
    - Budgeting apps: Income variability (>0.3: +0.3), low buffer (<30 days: +0.3), expense volatility (+0.2)
    - Subscription managers: High recurring merchants (>=5: +0.4), high spend share (>0.2: +0.3)
    - Investment products: High income (>$5k/mo) + low utilization (<0.3: +0.4), good emergency fund (>=3 months: +0.3), penalizes existing investment (-0.4)
  - Rationale generation: `generate_product_rationale()` function with category-specific templates:
    - Balance transfer: Cites utilization % and estimated monthly interest savings
    - HYSA: Cites monthly savings amount, APY, and annual interest earnings
    - Budgeting apps: Cites income variability or buffer days
    - Subscription managers: Cites recurring merchant count and monthly spend
    - Investment products: Cites monthly income and emergency fund months
  - Main matching function: `match_products()` function:
    - Parses persona_type (handles 30d/180d suffixes)
    - Queries active products where persona_targets contains user's persona
    - Calculates relevance scores for each candidate product
    - Generates personalized rationales citing user data
    - Filters products with relevance_score >= 0.5
    - Sorts by score descending, returns top 3 products
    - Comprehensive logging for matched products and scores
  - Test script created (`scripts/test_product_matching.py`):
    - Tests all 5 persona types (high_utilization, savings_builder, variable_income, subscription_heavy, wealth_builder)
    - Verifies category-specific products ranked high for each persona
    - Tests edge cases (no matching products, low relevance scores)
    - Verifies rationale specificity (cites specific user data with numbers/percentages)
    - Comprehensive test coverage for all matching scenarios
  - All 91 tasks completed (service setup, scoring logic, rationale generation, main function, helper functions, testing)
- ✅ **PR #38 Complete: Database Schema & Product Catalog Generation (all 64 tasks finished)**
- ✅ **PR #39 Complete: Product Ingestion via API (all tasks finished)**
  - Added products support to `/ingest/` endpoint for consistency with other data ingestion
  - Created `ProductCreate` and `ProductResponse` schemas in `backend/app/schemas.py`
  - Updated `IngestRequest` to include `products: List[ProductCreate]`
  - Updated ingestion endpoint (`backend/app/routers/ingest.py`) to handle products:
    - Converts `persona_targets` and `benefits` lists to JSON strings
    - Bulk inserts products into database
    - Includes products count in `IngestResponse`
  - Created `scripts/test_ingest_products.py` for API-based product ingestion
  - Removed direct database seeding scripts (`seed_product_catalog.py`, `test_product_seeding.py`)
  - Updated README.md to document API-based ingestion workflow
  - Products now follow same ingestion pattern as users, accounts, transactions, liabilities
  - Created `ProductOffer` model in `backend/app/models.py` with all required fields:
    - Core fields: product_id, product_name, product_type, category, persona_targets (JSON)
    - Eligibility criteria: min_income, max_credit_utilization, requires_no_existing_savings/investment, min_credit_score
    - Content fields: short_description, benefits (JSON), typical_apy_or_fee, partner_link, disclosure
    - Business fields: partner_name, commission_rate, priority, active
    - Timestamps: created_at, updated_at (auto-set/updated)
    - Indexes: persona_targets, active status
  - Product generation script (`scripts/generate_product_catalog.py`):
    - Enhanced to load `.env` from backend folder (no separate .env needed)
    - Uses OpenAI GPT-4o to generate 20-25 realistic financial products
    - Prompt includes all 5 personas, eligibility criteria guidelines, standard disclosure template
    - Validation and enhancement functions add all required metadata fields
    - Generated 21 products successfully (within 20-25 target)
  - Product catalog generated (`data/product_catalog.json`):
    - 21 products covering all 5 personas (high_utilization: 5, variable_income: 7, subscription_heavy: 4, savings_builder: 6, wealth_builder: 7)
    - Categories: balance_transfer, hysa, budgeting_app, subscription_manager, robo_advisor, investment_account, retirement_plan, debt_consolidation
    - All products include disclosures, 3-5 benefits, realistic APY/fee values, appropriate eligibility criteria
    - Product names realistic (Chase, Marcus, YNAB, Rocket Money, etc.)
  - Documentation:
    - Added product catalog generation instructions to README.md (expected runtime ~30-60s, cost ~$0.10-0.20)
    - Created `docs/PRODUCT_SCHEMA.md` with complete schema documentation, JSON format, eligibility guidelines, persona targeting
    - Updated project structure in README.md to include product_catalog.json
  - Test script created (`scripts/test_product_offer_model.py`) for model validation
  - All 64 tasks completed (database migration, model creation, script review, catalog generation, documentation)
- ✅ **Article Catalog Feature Breakdown Complete (tasks-12.md)**
  - Created comprehensive 275+ task breakdown across 6 PRs (PR #46-51)
  - Vector-based article matching using Pinecone + OpenAI embeddings
  - Real-time similarity search (<300ms for 2-3 recs)
  - 50 LLM-generated articles (10 per persona) - **Fake articles for MVP**
  - Note: Articles are placeholders, will replace with real content when time permits
  - Similarity threshold: 0.75 (show article only if highly relevant)
  - Frontend display: "Read Full Article" button with article metadata
  - 15+ unit tests planned for matching and performance
  - Expected timeline: 2.5-3.5 hours total implementation
  - Expected cost: ~$0.30-0.50 for one-time LLM-generated article catalog
- ✅ **Product Catalog Feature Breakdown Complete (tasks-10.md)**
  - Created comprehensive 410+ task breakdown across 8 PRs (PR #38-45)
  - Database schema designed for product_offers table with eligibility criteria
  - Product generation script enhanced with validation and better prompting
  - Product matching service architecture defined (persona + signal based relevance scoring)
  - ✅ Eligibility filtering logic implemented (PR #41 complete)
  - Hybrid recommendation engine planned (2-3 educational + 1-2 product offers)
  - Frontend display components specified (benefits, partner links, disclosure)
  - 20+ unit tests planned for product matching and eligibility
  - Expected timeline: 4-6 hours total implementation
  - Expected cost: ~$0.20 for one-time LLM-generated product catalog
- ✅ **Performance Testing Complete: OpenAI Latency Investigation**
  - Investigated 20-second average recommendation generation latency
  - Implemented detailed timing logs (SQL query, OpenAI query, tone validation, DB save)
  - Identified OpenAI API call as primary bottleneck (~17,000ms out of ~17,081ms total)
  - Tested three optimization strategies:
    1. **Model Change (gpt-3.5-turbo)**: 67% faster (17s → 5.5s) - MOST EFFECTIVE
    2. **Data Reduction**: No improvement (18.8s) - context size not a factor
    3. **Remove JSON Mode**: 35% slower (23s) - JSON mode is beneficial
  - Key Finding: Model choice (gpt-4o-mini vs gpt-3.5-turbo) is the only significant performance factor
  - Documentation: Created comprehensive testing documentation in `docs/performance_testing/`
    - `OPENAI_LATENCY_TESTING.md`: Full overview of testing methodology, results, and conclusions
    - `recommendation_timing_results.md`: Detailed results for each strategy
  - Timing log code commented out in `backend/app/routers/recommendations.py` for future use
  - Decision: Keeping gpt-4o-mini for quality over speed in MVP
- ✅ **PR #28 Complete: Evaluation Script - Metrics Computation (all 54 tasks finished)**
  - Created `scripts/evaluate.py` evaluation script
  - Coverage metrics: Computes percentage of users with personas assigned
  - Explainability metrics: Computes percentage of recommendations with rationale
  - Latency metrics: Calculates average and p95 recommendation generation time using numpy.percentile()
  - Auditability metrics: Verifies all recommendations have decision traces (100% auditability)
  - Persona distribution: Groups personas by type for 30d and 180d windows
  - Recommendation status breakdown: Groups recommendations by status
  - Main evaluation function: `run_evaluation()` generates unique run_id, calls all metric functions, combines results, prints formatted output
  - Database persistence: `save_evaluation_metrics()` saves all metrics to evaluation_metrics table with JSON details
  - Dependencies added: pandas==2.1.4, numpy==1.26.2 to requirements.txt
  - Script tested: Successfully runs, computes metrics, saves to database
  - Output: Formatted console output with all metrics, persona distributions, and status breakdowns
- ✅ **PR #27 Complete: Frontend - User Dashboard & Consent (all 44 tasks finished)**
  - Backend consent endpoints created:
    - Created `backend/app/routers/consent.py` router with POST `/consent` and GET `/consent/{user_id}` endpoints
    - Added consent schemas to `backend/app/schemas.py`: ConsentRequest, ConsentHistoryItem, ConsentResponse
    - POST `/consent` endpoint: Updates user consent status (grant/revoke), creates ConsentLog entries for audit trail
    - GET `/consent/{user_id}` endpoint: Returns consent status, timestamps, and full consent history
    - Router registered in `backend/app/main.py`
    - Error handling: 404 for user not found, 500 for server errors, comprehensive logging
  - Frontend components created:
    - ConsentToggle component (`frontend/src/components/ConsentToggle.jsx`): Switch component with confirmation dialogs, status badge, last updated timestamp, grant/revoke flows
    - UserRecommendationCard component (`frontend/src/components/UserRecommendationCard.jsx`): Read-only recommendation display with markdown rendering, expandable content/rationale, persona badge
  - UserDashboard page (`frontend/src/pages/UserDashboard.jsx`):
    - Data fetching: Parallel fetch of user, consent, and recommendations on mount
    - Consent management: Integrated ConsentToggle with grant/revoke flows, automatic recommendation fetch after grant, recommendations cleared on revoke
    - Recommendations display: Shows approved recommendations when consent granted, expandable content, markdown rendering
    - Empty states: No consent CTA card, consent granted but no recommendations "coming soon" message
    - Error handling: Error alerts with retry capability
    - Responsive layout: Mobile-friendly grid layout
    - Loading states: Skeleton loaders during data fetch
  - All 44 tasks completed (7 backend + 37 frontend)
- ✅ **Server Concurrency Optimizations Complete**
  - Implemented uvicorn workers (4 workers) for concurrent request handling
  - Enabled SQLite WAL (Write-Ahead Logging) mode for concurrent reads during writes
  - Fixed hanging issue: User list no longer hangs when recommendation generation is running
  - Workers allow multiple requests to be processed concurrently even with blocking operations
  - WAL mode allows database reads while writes are in progress
  - Documentation updated: README.md, techContext.md, DECISIONS.md, LIMITATIONS.md, FRAMEWORK_CONCURRENCY_COMPARISON.md
  - Location: `backend/app/database.py` (WAL mode), server startup command (workers)
- ✅ PR #3 Complete: Database Schema & SQLAlchemy Models (all 58 tasks finished)
  - Database configuration complete (SQLite setup with SQLAlchemy)
  - All 10 SQLAlchemy models implemented:
    - User model (user_id, full_name, email, consent fields, user_type)
    - Account model (account_id, user_id, type, balances, currency)
    - Transaction model (transaction_id, account_id, user_id, date, amount, merchant, categories)
    - Liability model (liability_id, account_id, APR fields, payment info)
    - UserFeature model (feature_id, user_id, window_days, behavioral signals)
    - Persona model (persona_id, user_id, persona_type, confidence_score)
    - Recommendation model (recommendation_id, user_id, content, status, approval fields)
    - EvaluationMetric model (metric_id, run_id, performance metrics)
    - ConsentLog model (log_id, user_id, action, timestamp)
    - OperatorAction model (action_id, operator_id, action_type, recommendation_id)
  - Database initialization on FastAPI startup
  - All tables verified with DB Browser for SQLite
  - Database file created: `backend/spendsense.db`
- ✅ PR #4 Complete: Pydantic Schemas for Data Validation (all 31 tasks finished)
  - Created `backend/app/schemas.py` with all validation schemas
  - User schemas: UserBase, UserCreate, UserResponse
  - Account schemas: AccountBase, AccountCreate, AccountResponse
  - Transaction schemas: TransactionBase, TransactionCreate, TransactionResponse (with date parsing)
  - Liability schemas: LiabilityBase, LiabilityCreate, LiabilityResponse (with date parsing)
  - Ingestion schemas: IngestRequest, IngestResponse
  - Feature schemas: UserFeatureResponse
  - Persona schemas: PersonaResponse
  - Recommendation schemas: RecommendationBase, RecommendationCreate, RecommendationResponse, RecommendationApprove, RecommendationOverride, RecommendationReject
  - All schemas use Pydantic v2 syntax with Literal types for enum validation
  - ORM compatibility configured with `from_attributes = True`
  - Validation tested and working
- ✅ PR #5 Complete: Data Ingestion API Endpoint (all 30 tasks finished)
  - FastAPI app updated with CORS middleware (localhost:5173, localhost:3000)
  - Created `backend/app/routers/ingest.py` with POST `/ingest` endpoint
  - Bulk ingestion implemented for all entity types:
    - Users: Bulk insert with transaction commit
    - Accounts: Bulk insert with transaction commit
    - Transactions: Batched processing (1000 per batch) for performance
    - Liabilities: Bulk insert with transaction commit
  - Error handling with rollback on failure
  - Idempotency handling for duplicate key errors (409 status)
  - Returns IngestResponse with counts and duration in milliseconds
  - Test script created (`scripts/test_ingest.py`)
  - All synthetic data successfully ingested:
    - 75 users loaded
    - 272 accounts loaded
    - 15,590 transactions loaded
    - 92 liabilities loaded
  - Data verified in database using SQLite browser
  - Swagger UI accessible at `/docs`
  - Requests dependency added (requests==2.31.0)
- ✅ PR #6 Complete: Feature Detection Service - Subscription Signals (all 22 tasks finished)
  - Created `backend/app/services/feature_detection.py` service file
  - Helper functions implemented:
    - `get_transactions_in_window()` - Queries transactions filtered by date window, ordered by date
    - `get_accounts_by_type()` - Queries accounts filtered by account types
  - Subscription detection implemented:
    - `compute_subscription_signals()` - Main function for subscription pattern detection
    - Groups transactions by merchant_name
    - Filters merchants with ≥3 transactions
    - `is_recurring_pattern()` - Detects recurring patterns:
      - Weekly subscriptions (~7 days ±5 tolerance)
      - Monthly subscriptions (~30 days ±5 tolerance)
      - Quarterly subscriptions (~90 days ±5 tolerance)
    - Calculates signals:
      - `recurring_merchants` (count of merchants with recurring patterns)
      - `monthly_recurring_spend` (sum of recurring transactions / months in window)
      - `subscription_spend_share` (recurring spend / total spend, 0-1 ratio)
  - Test script created (`scripts/test_feature_detection.py`)
  - Tests subscription detection for multiple users with both 30-day and 180-day windows
  - Logs results with merchant examples for validation
- ✅ PR #7 Complete: Feature Detection Service - Savings Signals (all 20 tasks finished)
  - Added `compute_savings_signals()` function to feature_detection.py
  - Savings account filtering:
    - Filters accounts by type: savings, money market, cash management, HSA
    - Returns zero values if no savings accounts found
  - Net inflow calculation:
    - Separates deposits (amount > 0) and withdrawals (amount < 0) per account
    - Calculates net_inflow per account (deposits + withdrawals)
    - Sums net_inflow across all savings accounts
    - Normalizes to monthly net inflow (net_inflow / months_in_window)
  - Growth rate calculation:
    - Calculates start balance (current balance - net inflow)
    - Computes growth rate per account: (current - start) / start
    - Averages growth rates across all savings accounts
  - Emergency fund calculation:
    - Calculates total savings balance across all savings accounts
    - Estimates monthly expenses from checking account transactions (expenses = amount < 0)
    - Calculates emergency_fund_months: savings_balance / avg_monthly_expenses
    - Handles edge case: sets to 0 if expenses = 0
  - Returns dict with:
    - `net_savings_inflow` (float, monthly average)
    - `savings_growth_rate` (float, 0-1, average across accounts)
    - `emergency_fund_months` (float, months of expenses covered)
  - Error handling: Division by zero protection, logging infrastructure
  - Test script updated to test savings detection for users with and without savings accounts
  - Validates growth rate and emergency fund calculations
- ✅ PR #8 Complete: Feature Detection Service - Credit Signals (all 21 tasks finished)
  - Added `compute_credit_signals()` function to feature_detection.py
  - Credit card account querying:
    - Queries credit card accounts using `get_accounts_by_type()` helper
    - Joins with liabilities table via account_id
    - Returns zero/false values if no credit cards found
  - Utilization calculation:
    - Calculates utilization per card: balance_current / balance_limit
    - Computes average utilization across all cards
    - Tracks max utilization (highest single card)
    - Sets utilization flags: ≥30%, ≥50%, ≥80% thresholds
  - Minimum payment detection:
    - Checks if last_payment <= minimum_payment (with $5 tolerance)
    - Sets `minimum_payment_only_flag` if any card matches pattern
  - Interest & overdue detection:
    - Queries transactions for interest charges using `category_detailed` filter (ilike '%interest%')
    - Checks `is_overdue` field on liabilities
    - Sets `interest_charges_present` and `any_overdue` flags
  - Returns dict with all 8 credit signals:
    - avg_utilization, max_utilization (float, 0-1)
    - utilization_30_flag, utilization_50_flag, utilization_80_flag (bool)
    - minimum_payment_only_flag, interest_charges_present, any_overdue (bool)
  - Error handling: Division by zero protection (when balance_limit = 0), debug logging
  - Test script updated (`scripts/test_feature_detection.py`) with comprehensive credit detection tests
  - Tests cover: high/low utilization, minimum payments, overdue accounts
  - Database path fix: Updated test script to use absolute path to `backend/spendsense.db`
- ✅ PR #9 Complete: Feature Detection Service - Income Signals (all 33 tasks finished)
  - Added `compute_income_signals()` function to feature_detection.py
  - Payroll identification:
    - Filters transactions for payroll deposits (ACH, positive amounts, income categories, or payroll merchant names)
    - Sorts transactions chronologically
    - Requires ≥2 payroll transactions to detect payroll
  - Income pattern analysis:
    - Calculates gaps between consecutive paydays
    - Computes median pay gap days using `statistics.median()`
    - Returns default values if <2 payroll transactions found
  - Income variability calculation:
    - Calculates mean and standard deviation of payroll amounts
    - Computes coefficient of variation (std_dev / mean)
    - Handles edge cases (mean = 0 or only 1 payment)
  - Cash flow buffer calculation:
    - Gets checking account balance
    - Estimates monthly expenses from checking account transactions
    - Calculates months of expenses covered by checking balance
    - Handles division by zero
  - Average monthly income:
    - Sums all payroll amounts in window
    - Divides by number of months, rounds to 2 decimal places
  - Investment account detection:
    - Added `detect_investment_accounts()` function
    - Queries for investment account types: brokerage, 401k, ira, roth_ira, investment, pension
    - Returns True/False if any exist
  - Test script updated with comprehensive income detection tests
- ✅ PR #10 Complete: Feature Computation Endpoint & Batch Script (all 28 tasks finished)
  - Created `compute_all_features()` function in feature_detection.py:
    - Calls all 4 signal detection functions (subscription, savings, credit, income)
    - Calls `detect_investment_accounts()`
    - Combines all results into single dict
    - Creates or updates UserFeature record in database
    - Returns computed features
  - Features router (`backend/app/routers/features.py`):
    - POST `/compute/{user_id}` endpoint with window_days query parameter (default: 30)
    - Error handling for user not found
  - Profile router (`backend/app/routers/profile.py`):
    - GET `/{user_id}` endpoint returns features (30d and/or 180d) and personas
    - Optional window filter
    - Error handling for user not found
  - Routers registered in main.py
  - Batch computation script (`scripts/compute_all_features.py`):
    - Processes all users, computes features for both 30-day and 180-day windows
    - Progress reporting every 10 users
    - Summary statistics (total users, avg time, duration)
  - Successfully created 142 feature records (71 users × 2 windows)
  - Average computation time: 0.013 seconds per user
- ✅ PR #11 Complete: Frontend - Project Setup & Basic Routing (all 39 tasks finished)
  - Vite configuration updated: Port set to 5173, path alias `@src` configured
  - API client setup (`frontend/src/lib/api.js`):
    - Axios instance with baseURL from environment variable
    - Request/response interceptors for logging and error handling
    - Default headers configured
  - API service functions (`frontend/src/lib/apiService.js`):
    - All 12 API functions created: getUsers, getUser, getUserProfile, getRecommendations, getOperatorDashboard, getOperatorUserSignals, getApprovalQueue, approveRecommendation, bulkApprove, updateConsent, getConsent
  - Routing setup (`frontend/src/App.jsx`):
    - React Router configured with BrowserRouter
    - Routes: / → /operator/dashboard, /operator/dashboard, /operator/users, /operator/users/:userId, /operator/approval-queue, /user/:userId/dashboard
  - Layout component (`frontend/src/components/Layout.jsx`):
    - Navigation header with active state styling
    - Links: Operator Dashboard, User List, Approval Queue
  - Page placeholders created:
    - OperatorDashboard, OperatorUserList, OperatorUserDetail, OperatorApprovalQueue, UserDashboard
  - Configuration files updated:
    - `vite.config.js`: Port 5173, `@src` alias with ES module path resolution
    - `tailwind.config.js`: ES module import for tailwindcss-animate plugin
    - `jsconfig.json`: Path alias `@src/*` for IDE resolution
- ✅ Python Environment Upgrade: Venv recreated with Python 3.11.9 (was 3.9.6)
  - All dependencies reinstalled with Python 3.11.9
  - VS Code settings updated with `python.analysis.diagnosticMode: "workspace"` for better import resolution
- ✅ PR #12 Complete: Frontend - Operator Dashboard (Metrics & Charts) (all 39 implementation tasks finished)
  - Dashboard data fetching (`frontend/src/pages/OperatorDashboard.jsx`):
    - useState/useEffect hooks for API data fetching
    - Loading, error, and data state management
    - Calls `getOperatorDashboard()` API function
    - Error handling with retry functionality
  - UI components created:
    - `MetricsCard.jsx`: Reusable metric card component using Shadcn Card
    - `skeleton.jsx`: Loading skeleton component for UI placeholders
    - `alert.jsx`: Error alert component with title and description
  - Dashboard layout:
    - Responsive grid layout (1 column mobile, 2 tablet, 4 desktop)
    - 4 metrics cards: Total Users, Users with Consent, Pending Approvals, Avg Latency
  - Charts implemented:
    - Persona Distribution chart: Bar chart showing persona counts using Recharts
    - Recommendation Status chart: Color-coded bar chart (pending=blue, approved=green, overridden=amber, rejected=red)
    - Data transformation from API objects to chart-friendly arrays
    - Responsive containers for all charts
  - Loading states:
    - Skeleton placeholders for metrics cards and charts
    - Loading state shown during data fetch
  - Error states:
    - Alert component displays error messages
    - Retry button allows re-fetching data
  - Fast Refresh fixes:
    - Removed "use client" directive (not needed in Vite/React)
    - Removed variant exports from badge.jsx and button.jsx (Fast Refresh requires component-only exports)
    - All imports updated to use `@src` alias consistently
- ✅ PR #13 Complete: Frontend - Operator User List Page & Backend Endpoints (all 61 tasks finished)
  - Frontend components created:
    - UserTable: Table component with columns (Name, Email, Persona 30d, Consent Status, Actions), colored badges for personas and consent, clickable rows
    - UserFilters: Filter dropdowns for user type and consent status using styled native selects
    - Pagination: Page navigation with previous/next buttons, page numbers (5 at a time), disabled states
    - UserTableSkeleton: Loading skeleton matching table structure
  - Frontend enum system:
    - Created `frontend/src/constants/enums.js` with UserType, ConsentStatus, ConsentAction enums
    - Helper functions for display formatting
    - Used throughout frontend for consistency (documented in systemPatterns.md)
  - OperatorUserList page:
    - State management: users, filteredUsers, loading, error, searchTerm, filters, pagination
    - Data fetching: useEffect with useCallback for fetchUsers, handles filters and pagination
    - Search: Debounced search (300ms) with local filtering by name or email
    - Loading states: Skeleton component during fetch
    - Error states: Alert component with retry button
    - Empty states: Different messages for search vs filter scenarios
    - Responsive layout: Flex layouts, mobile-friendly
  - Backend endpoints:
    - GET /users: Pagination (limit/offset), filters (user_type, consent_status), includes 30d personas for each user, returns users array with total count
    - GET /operator/dashboard: Total users, users with consent, persona distribution (30d), recommendation status breakdown, average latency metrics
  - Router registration: Both routers registered in main.py
  - All functionality tested and working
- ✅ PR #14 Complete: Frontend - Operator User Detail Page (all 52 implementation tasks finished)
  - Backend endpoints added:
    - GET /users/{user_id}: Returns user with personas for both 30d and 180d windows
    - GET /operator/users/{user_id}/signals: Returns detailed signals for operator view with:
      - 30d_signals and 180d_signals objects containing subscriptions, savings, credit, income
      - Recurring merchant names (array), credit card details (per-card), income frequency
      - Uses UserFeature data for most fields, queries additional details as needed
  - Frontend components created:
    - UserInfoCard: Displays user name, email, user type, consent status with badges, consent dates
    - PersonaDisplay: Shows persona type with large badge, confidence score as percentage, assigned date, color-coded by persona type
    - SignalDisplay: Comprehensive signal visualization component with 4 views:
      - Subscriptions: Recurring merchants count, monthly spend, spend share with progress bar, merchant list
      - Savings: Net inflow, growth rate, emergency fund coverage with color-coded progress bar (red/yellow/green)
      - Credit: Avg/max utilization with progress bars, per-card details (last_four, utilization, balance, limit), warning badges for flags
      - Income: Payroll detection badge, avg monthly income, pay gap, variability, cash flow buffer with warnings
    - Progress component: Shadcn-style progress bar component created
  - OperatorUserDetail page:
    - Two-column responsive layout (left: user info + personas, right: signals with tabs)
    - Tab navigation for signal types (subscriptions, savings, credit, income)
    - Displays signals for both 30d and 180d windows
    - Recommendations section: Fetches and displays recommendations with status badges, "Generate Recommendations" button placeholder
    - Back navigation button linking to user list
    - Loading states: Skeleton placeholders for all sections
    - Error states: Alert component with retry functionality
    - Data fetching: Parallel API calls for user, profile, signals, and recommendations
- ✅ **PR #15 Complete: Persona Assignment Service (all 29 tasks finished)**
  - Created `backend/app/services/persona_assignment.py` service file
  - Persona check functions implemented:
    - `check_high_utilization()`: Returns True if max_utilization >= 0.50 OR interest_charges_present OR minimum_payment_only_flag OR any_overdue
    - `check_variable_income()`: Returns True if median_pay_gap_days > 45 AND cash_flow_buffer_months < 1
    - `check_subscription_heavy()`: Returns True if recurring_merchants >= 3 AND (monthly_recurring_spend >= 50 OR subscription_spend_share >= 0.10)
    - `check_savings_builder()`: Returns True if (savings_growth_rate >= 0.02 OR net_savings_inflow >= 200) AND avg_utilization < 0.30
    - `check_wealth_builder()`: Returns True if avg_monthly_income > 10000 AND savings_balance > 25000 AND max_utilization <= 0.20 AND no overdrafts/late fees AND investment_account_detected
  - Helper functions:
    - `get_total_savings_balance()`: Queries savings accounts and sums balance_current
    - `has_overdraft_or_late_fees()`: Checks transactions for fee-related categories in window
  - Persona assignment logic (`assign_persona()`):
    - Queries UserFeature for user and window
    - Checks personas in priority order: wealth_builder (1.0), high_utilization (0.95 if util>=80%, else 0.8), savings_builder (0.7), variable_income (0.6), subscription_heavy (0.5)
    - Returns highest priority persona with confidence score and reasoning dictionary
    - Reasoning dict includes: matched_criteria, feature_values, timestamp, priority, all_matched_personas
  - Persona persistence (`create_or_update_persona()`):
    - Checks if Persona record exists for user + window
    - Updates existing or creates new record
    - Serializes reasoning dict to JSON string
    - Commits to database
  - Main function (`assign_and_save_persona()`):
    - Calls assign_persona() to get type, confidence, reasoning
    - Calls create_or_update_persona() to save
    - Returns Persona object
  - Test script created (`scripts/test_persona_assignment.py`):
    - Tests individual persona check functions
    - Tests persona assignment logic
    - Tests saving personas to database
    - Verifies persona records in database
    - Shows persona distribution statistics
- ✅ **PR #16 Complete: Persona Assignment Endpoint & Batch Script (all 29 tasks finished)**
  - Personas router (`backend/app/routers/personas.py`):
    - POST `/{user_id}/assign` endpoint: Assigns persona for user with optional window_days query parameter (default: 30)
    - GET `/{user_id}` endpoint: Retrieves personas for user with optional window filter
    - Error handling: 404 (user not found), 400 (features not computed), 500 (server error)
    - Returns PersonaResponse objects
  - Router registration: Personas router registered in main.py
  - Batch assignment script (`scripts/assign_all_personas.py`):
    - Processes all users, assigns personas for both 30-day and 180-day windows
    - Progress reporting every 10 users
    - Summary statistics: Total users processed, persona distribution for 30d and 180d windows, fallback assignments tracked separately
    - Validation warnings for missing persona types
  - Fallback behavior: When no persona matches, assigns 'savings_builder' with low confidence (0.1 if no features, 0.2 if features exist but no match)
  - Successfully assigned personas to 71 users (142 persona records: 71 users × 2 windows)
  - Note: Some persona types (wealth_builder, variable_income) not represented in current synthetic data - will enhance data generation later for better variance
- ✅ **PR #17 Complete: OpenAI Integration Setup & Prompt Templates (all 50 tasks finished)**
  - OpenAI dependencies:
    - Added `openai==1.3.5` to `backend/requirements.txt`
    - Installed OpenAI SDK in virtual environment
    - API key configured in `.env.local` (already present)
    - `.gitignore` already covers `.env` files
  - Prompts infrastructure:
    - Created `backend/app/prompts/` directory with `__init__.py`
    - Created 5 self-contained persona-specific prompt files (following "just right" calibration guide)
    - Each prompt is ~42-52 lines (lean and focused)
    - Prompts follow structure: Role & Context, Core Principles, Response Framework, Guidelines, Output Format
  - Persona-specific prompts created:
    - `high_utilization.txt`: Credit card debt management strategies
    - `variable_income.txt`: Irregular income budgeting and cash flow strategies
    - `subscription_heavy.txt`: Subscription optimization and audit strategies
    - `savings_builder.txt`: Savings acceleration and goal-setting strategies
    - `wealth_builder.txt`: Investment/retirement education (with restrictions on specific investment advice)
  - Each prompt includes:
    - Core principles (regulatory compliance, data citation, empowering tone)
    - 5-step response framework for generating recommendations
    - Persona-specific guidelines with topic list ("When relevant, address topics like...")
    - JSON output format specification with mandatory disclosure
  - Prompt loader utility (`backend/app/utils/prompt_loader.py`):
    - `load_prompt(persona_type: str) -> str` function
    - File path construction and error handling
    - In-memory caching for performance
    - Helper functions for cache management
  - Prompt design decisions:
    - Self-contained prompts (no base template) for clarity and maintainability
    - Reduced prescription, increased principles (following "just right" calibration guide)
    - Simplified output format (no lengthy examples)
    - Topic lists included in guidelines for persona-specific depth
- ✅ **PR #18 Complete: Recommendation Engine Service - Context Building (all 34 tasks finished)**
  - Recommendation engine service (`backend/app/services/recommendation_engine.py`):
    - OpenAI client setup function (`get_openai_client()`) with API key from environment
    - User context builder (`build_user_context()`) that queries:
      - User record (user_id, basic info)
      - UserFeature record for window (all behavioral signals)
      - Persona record for window (persona_type)
      - Account records (top 5 by balance, with masked names like "Checking ****1234")
      - Recent transactions (last 10, last 30 days, with date, merchant, amount, type)
      - High utilization credit cards (when utilization ≥ 50%, includes balance, limit, utilization %, interest estimates)
      - Recurring merchants list (merchants with 3+ transactions in window)
      - Savings account growth info (when applicable, includes count, total balance, growth rate, emergency fund months)
    - Context structure:
      - Base info: user_id, window_days, persona_type
      - Subscription signals: recurring_merchants, monthly_recurring_spend, subscription_spend_share
      - Savings signals: net_savings_inflow, savings_growth_rate, emergency_fund_months
      - Credit signals: avg_utilization, max_utilization, utilization flags, payment patterns, interest/overdue status
      - Income signals: payroll_detected, median_pay_gap_days, income_variability, cash_flow_buffer_months, avg_monthly_income
      - Accounts: List of top 5 accounts with masked names and balances
      - Recent transactions: Last 10 transactions with formatted dates and amounts
      - Optional: high_utilization_cards, recurring_merchants, savings_accounts (when applicable)
    - Context validation (`validate_context()`):
      - Checks all required fields present
      - Validates data types
      - Logs errors for debugging
    - All float values rounded to 2 decimal places for readability
    - Token efficiency: Limits to top 5 accounts, last 10 transactions, top 10 recurring merchants
  - Test script (`scripts/test_context_builder.py`):
    - Tests context building for multiple users
    - Validates context structure and required fields
    - Checks data quality and realism
    - Estimates token count (target <2000 tokens)
    - Provides detailed output for review
  - Test results: All 5 users tested successfully, context validation passed, token counts well under target (583-764 tokens)
- ✅ **PR #19 Complete: Recommendation Engine Service - OpenAI Integration (all 26 tasks finished)**
  - OpenAI API integration (`generate_recommendations_via_openai()` function):
    - Loads persona-specific prompts using prompt_loader
    - Converts user context to JSON string
    - Calls OpenAI chat completions API (gpt-4o-mini, temperature 0.75, JSON response format)
    - Implements exponential backoff retry logic for rate limits (3 retries, 1s/2s/4s delays)
    - Comprehensive error handling (rate limits, invalid API key, model not found, JSON parsing)
    - Extracts and parses JSON response with validation
    - Validates required fields (title, content, rationale) for each recommendation
    - Adds generation_time_ms and persona_type to each recommendation
    - Token usage tracking (extracted from response, logged, calculated cost)
    - Token info included in metadata for review (NOT saved to DB - stripped before DB save)
  - Prompt improvements:
    - Added LANGUAGE STYLE section to all 5 persona prompts
    - Explicit requirements for empowering phrases: "You can...", "Let's explore...", "Many people find...", "Consider..."
    - Requirements to frame as opportunities and normalize challenges
    - Avoid directive language ("You should...", "You must...")
    - Temperature increased from 0.7 to 0.75 for more natural variation
  - Test script (`scripts/test_openai_generation.py`):
    - Tests all 5 persona types
    - Validates JSON response structure, recommendation count (3-5), required fields
    - Verifies rationales cite specific data
    - Checks content quality and tone (shaming language detection, empowering language verification)
    - Measures latency (target <5s, actual 9-22s acceptable for MVP)
    - Saves complete output to `openai_test_output.json` with token usage, costs, and recommendations
  - Test results: All 3 tested persona types passed quality checks, all recommendations use empowering language
  - OpenAI SDK upgraded from 1.3.5 to 2.7.1 for compatibility
- ✅ **PR #20 Complete: Guardrails Service - Tone & Consent Validation (all 38 tasks finished)**
  - Guardrails service (`backend/app/services/guardrails.py`):
    - `validate_tone()` function: Returns structured dict with `is_valid` and `validation_warnings` array
      - Critical warnings: Forbidden phrases (severity="critical", type="forbidden_phrase") → RED in operator UI
      - Notable warnings: Lacks empowering language (severity="notable", type="lacks_empowering_language") → YELLOW in operator UI
      - Empty array when valid, populated array with warnings when invalid
    - `check_consent()` function: Checks user consent status with logging
    - Eligibility functions: `check_income_eligibility()`, `check_credit_eligibility()`, `check_account_exists()`
    - `filter_partner_offers()` function: Filters offers based on eligibility requirements
    - `append_disclosure()` function: Appends mandatory disclosure to content
    - `MANDATORY_DISCLOSURE` constant defined
  - Test script (`scripts/test_guardrails.py`):
    - Tests tone validation with valid content, forbidden phrases, lacks empowering language, both issues
    - Tests consent checking with consented/non-consented/non-existent users
    - Tests eligibility checks (income, credit, account existence)
    - Tests partner offer filtering
    - Tests mandatory disclosure appending
  - Test results: All tests passing, all 38 tasks completed
  - Key design: All recommendations persisted regardless of warnings - operator reviews and decides
- ✅ **PR #21 Complete: Recommendation Generation Endpoint (all 44 tasks finished)**
  - Recommendations router (`backend/app/routers/recommendations.py`):
    - POST `/recommendations/generate/{user_id}` endpoint with full workflow
    - Query parameters: `window_days` (default: 30), `force_regenerate` (default: False)
    - User validation (404 if not found), consent check (403 if not consented)
    - Existing recommendations check (returns cached if exists and not force_regenerate)
    - Persona validation (400 if no persona assigned)
    - Context building and validation
    - OpenAI integration for recommendation generation
    - Tone validation for each recommendation with warnings stored in metadata
    - Database persistence with status='pending_approval' for all recommendations
    - Mandatory disclosure appended to content
    - Validation warnings stored in metadata_json (empty array for valid, populated for invalid)
    - Token usage and cost data stripped from metadata before saving (logging only)
    - Status codes: 200 for cached results, 201 for newly created
    - Comprehensive error handling with rollback on failures
  - Router registration: Recommendations router registered in main.py
  - Metadata design: Saves metadata (including validation_warnings) but excludes token_usage and estimated_cost_usd (used for logging/review only)
  - All recommendations persisted regardless of warnings for operator review
  - Tested and verified via Swagger UI
- ✅ **PR #22 Complete: Get Recommendations Endpoint (all 23 tasks finished)**
  - Recommendations router (`backend/app/routers/recommendations.py`):
    - GET `/recommendations/{user_id}` endpoint implemented
    - Query parameters: `status` (optional, filters by status), `window_days` (optional, filters by window_days)
    - User validation (404 if not found)
    - Query logic: Filters by user_id, optional status, optional window_days
    - Ordering: By generated_at descending (newest first)
    - Limit: 50 recommendations (pagination ready for future)
    - Response format: Returns recommendations list with all required fields and total count
    - Access control: Removed premature access control logic (will be implemented when authentication is added)
    - Error handling: 404 for user not found, 500 for database errors, comprehensive logging
    - Test script created (`scripts/test_get_recommendations.py`) for testing various filter combinations
  - Note: Access control removed - endpoint returns all recommendations when no status filter provided (authentication needed to determine requester identity)
- ✅ **PR #23 Complete: Approve Recommendation Endpoint (all 26 tasks finished)**
  - Recommendations router (`backend/app/routers/recommendations.py`):
    - POST `/recommendations/{recommendation_id}/approve` endpoint implemented
    - Accepts recommendation_id as path parameter
    - Accepts RecommendationApprove schema in body (operator_id, optional notes)
    - Validation: 404 if recommendation not found, 400 if already approved, 400 if status is 'rejected'
    - Updates recommendation: Sets status='approved', approved_by=operator_id, approved_at=current timestamp
    - Creates OperatorAction record for audit trail (action_type='approve', includes operator_id, recommendation_id, user_id, reason)
    - Returns updated recommendation with all fields
    - Error handling: 404 for not found, 400 for invalid state transitions, 500 for database errors
    - Comprehensive logging for all approve actions
  - Test script created (`scripts/test_approve_recommendation.py`) for testing approval workflow
  - All 26 tasks completed
- ✅ **PR #24 Complete: Override & Reject Endpoints (all 39 tasks finished)**
  - Recommendations router (`backend/app/routers/recommendations.py`):
    - POST `/recommendations/{recommendation_id}/override` endpoint implemented
    - POST `/recommendations/{recommendation_id}/reject` endpoint implemented
  - Schema updates:
    - Updated `RecommendationOverride` schema to make `new_title` and `new_content` optional
  - Override endpoint features:
    - Accepts recommendation_id as path parameter
    - Accepts RecommendationOverride schema (operator_id, optional new_title, optional new_content, required reason)
    - Validation: 404 if not found, 400 if neither new_title nor new_content provided
    - Stores original content in JSON format (original_title, original_content, overridden_at timestamp)
    - Updates recommendation: Sets status='overridden', updates title/content if provided, appends disclosure, validates tone
    - Tone validation: Rejects new content with critical warnings (forbidden phrases) → 400 error
    - Creates OperatorAction record with action_type='override'
    - Returns updated recommendation with original_content and override_reason
  - Reject endpoint features:
    - Accepts recommendation_id as path parameter
    - Accepts RecommendationReject schema (operator_id, required reason)
    - Validation: 404 if not found, 400 if already approved (can't reject approved recs)
    - Updates recommendation: Sets status='rejected', stores rejection reason in metadata_json
    - Metadata includes: rejection_reason, rejected_by, rejected_at
    - Creates OperatorAction record with action_type='reject'
    - Returns updated recommendation with rejection metadata
  - Error handling: 404 for not found, 400 for validation errors, 500 for database errors
  - Comprehensive logging for all override and reject actions
  - Test script created (`scripts/test_override_reject.py`) with 8 test cases
  - All 39 tasks completed
- ✅ **PR #25 Complete: Bulk Approve Endpoint (all 21 tasks finished)**
  - Recommendations router (`backend/app/routers/recommendations.py`):
    - POST `/recommendations/bulk-approve` endpoint implemented
    - Accepts BulkApproveRequest schema (operator_id, recommendation_ids array)
    - Processes each recommendation individually with error handling
    - Validates each recommendation exists and is in 'pending_approval' status
    - Updates recommendations: Sets status='approved', approved_by=operator_id, approved_at=current timestamp
    - Creates OperatorAction records for audit trail
    - Batch commit: All changes committed in single transaction
    - Returns BulkApproveResponse with approved count, failed count, and error messages
    - Error handling: Individual recommendation errors don't fail entire batch
    - Status codes: 200 if any succeeded, 400 if all failed
    - Comprehensive logging for bulk operations
  - Test script created (`scripts/test_bulk_approve.py`) for testing bulk approval workflow
  - All 21 tasks completed
- ✅ **PR #26 Complete: Frontend - Approval Queue Page (all 74 tasks finished)**
  - Backend endpoint added:
    - GET `/operator/review` endpoint: Fetches all recommendations where status != 'approved'
    - Optional status filter (pending_approval, overridden, rejected)
    - Orders by generated_at ascending (oldest first, queue order)
    - Includes user information via JOIN for efficient querying
    - Returns recommendations with all relevant fields and total count
  - Frontend components created:
    - RecommendationCard component: Displays recommendation with checkbox, user info, persona badge, content/rationale previews, action buttons (Approve, Reject, Override)
    - Checkbox UI component: Created using Radix UI (@radix-ui/react-checkbox)
  - API service functions added:
    - `overrideRecommendation()` - Override a recommendation with new content
    - `rejectRecommendation()` - Reject a recommendation with reason
  - OperatorApprovalQueue page:
    - Data fetching with useState/useEffect, auto-refresh every 30 seconds
    - State management: recommendations, selectedIds (Set), loading, error, statusFilter, actionLoading
    - Bulk selection: Select all checkbox, individual checkboxes, selected count display
    - Bulk approve button: Disabled when no selection, loading state, success/error messaging
    - Individual approve: Per-recommendation approve with loading state, removes from queue on success
    - Override dialog: Shadcn Dialog with form fields (new title optional, new content optional, reason required), client-side validation with inline error messages
    - Reject dialog: Shadcn Dialog with reason field (required), client-side validation with inline error messages
    - Page layout: Header with title and refresh button, toolbar with select all, count, filter dropdown, bulk approve button
    - Status filter: Dropdown for pending_approval, overridden, rejected
    - Loading states: Skeleton cards while loading
    - Error states: Alert component with retry functionality
    - Success/error messages: Auto-dismiss after 5 seconds
    - Empty state: Message when no recommendations in queue
    - Auto-refresh: 30-second interval with cleanup on unmount
  - Server concurrency optimizations:
    - Implemented uvicorn workers (4 workers) for concurrent request handling
    - Enabled SQLite WAL (Write-Ahead Logging) mode for concurrent reads during writes
    - Fixed hanging issue: User list no longer hangs when recommendation generation is running
    - Documentation updated: README.md, techContext.md, DECISIONS.md, LIMITATIONS.md, FRAMEWORK_CONCURRENCY_COMPARISON.md
  - Markdown rendering for recommendations:
    - Installed react-markdown package for rendering Markdown in recommendation content
    - Updated RecommendationCard component to use ReactMarkdown for content and rationale previews
    - Updated OperatorUserDetail page to use ReactMarkdown for recommendation content display
    - Added type checking to ensure ReactMarkdown always receives string values (handles undefined/null)
    - Updated truncateText helper to validate input is a string before processing
    - Markdown syntax (bold, italic, etc.) renders correctly in both approval queue and user detail pages
  - Expandable content functionality:
    - Added expandable content to RecommendationCard component with independent state for content and rationale
    - Added expandable content to OperatorUserDetail recommendation cards using Set-based state tracking
    - "Show more"/"Show less" buttons appear only when content/rationale exceeds 150 characters
    - Full content expands inline with ReactMarkdown rendering
    - Both content and rationale can be expanded independently in RecommendationCard
    - Styled expand/collapse buttons with blue hover states (text-blue-600 hover:text-blue-800 hover:underline)
  - Form validation improvements:
    - Added client-side validation for override and reject dialogs
    - Validation errors display as red italic text below input fields (not in global error state)
    - Errors persist until user types in the field
    - Modal stays open and prevents API call if validation fails
    - Separate error state for each form (overrideFormError, rejectFormError)
  - All 74 tasks completed

## Next Steps
1. **Article Catalog Implementation** - PR #46-51: Add educational article recommendations via vector similarity search
   - Generate 50 LLM-generated articles (10 per persona) for MVP
   - Implement vector-based article matching using Pinecone
   - Real-time similarity search (<300ms for 2-3 recs)
   - 0.75 similarity threshold (show only highly relevant articles)
   - Frontend display with "Read Full Article" button
   - Add 15+ unit tests for matching and performance
   - **Note**: Articles are placeholders, will replace with real content later
2. **Product Catalog Implementation** - PR #38-45: Add product recommendations alongside educational content
   - ✅ **PR #38 Complete**: Database schema, product generation, catalog created (21 products)
   - ✅ **PR #39 Complete**: Product ingestion via API endpoint (consistent with other data)
   - ✅ **PR #40 Complete**: Product matching service (persona + signal based scoring, rationale generation, top 3 products)
   - ✅ **PR #41 Complete**: Eligibility filtering (income, utilization, existing accounts, category-specific rules)
   - 🔄 **PR #42-45**: Hybrid recommendation engine, frontend display, product management API, unit tests
   - **PAUSED AWS tasks** - No AWS access currently, implementing features instead
2. ✅ **Parquet Export & S3 Integration** - PR #29: Export user features and evaluation results to Parquet, upload to S3 (COMPLETE)
3. ✅ **Evaluation API Endpoint** - PR #30: Create API endpoints for running evaluations and retrieving metrics (COMPLETE)
4. **Redis Caching Layer** - PR #31: Implement multi-tier Redis caching for all DB queries and API responses
5. **PostgreSQL Migration** - PR #32: Migrate from SQLite to PostgreSQL (AWS RDS) for production (PAUSED - no AWS access)
6. **Scale Synthetic Data** - Future PR: Generate 500-1,000 users with recommendations (prerequisite for vector DB)
7. **Vector DB Integration** - PR #34-37: Integrate Pinecone Serverless with OpenAI embeddings for sub-1s latency
8. **AWS Deployment** - Deploy backend to Lambda and frontend to Vercel/Netlify (PAUSED - no AWS access)

## Deployment Status

### Frontend Deployment
- ✅ **Netlify Deployment Complete**
  - Frontend deployed to Netlify
  - `netlify.toml` configuration file created in root directory
  - Base directory: `frontend`
  - Build command: `npm run build`
  - Publish directory: `dist` (relative to base)
  - Node version: 18
  - Client-side routing configured with redirect rule (/* → /index.html)
  - Root directory in Netlify UI: Leave blank/empty

## Active Decisions and Considerations

### Immediate Priorities
- **Article Catalog Feature Implementation** (PR #46-51)
  - **Current Focus**: Implementing educational article recommendations via vector similarity search
  - **Why**: Enhances educational recommendations with external resources, perfect use case for vector DB
  - **Impact**: Each educational rec gets linked article (if similarity >0.75), better user experience
  - **Timeline**: 2.5-3.5 hours across 6 PRs (275+ tasks in tasks-12.md)
  - **Approach**: LLM-generated articles (MVP), Pinecone vector similarity search, real-time matching
  - **Performance**: +300ms to rec generation (acceptable), much better than LLM-based matching
  - **Testing**: 15+ unit tests planned for matching and performance
  - **Note**: Using fake articles for MVP (tight schedule), will replace with real articles when time permits
- **Product Catalog Feature Implementation** (PR #38-45)
  - **Status**: Planning complete, ready for implementation after article catalog
  - **Impact**: Enables hybrid recommendations (2-3 educational + 1-2 product offers per user)
  - **Timeline**: 4-6 hours across 8 PRs (410+ tasks in tasks-10.md)
  - **Approach**: LLM-generated product catalog (GPT-4o), rule-based matching + eligibility filtering
  - **Testing**: 20+ unit tests planned for product matching and eligibility logic
- **AWS Tasks Paused**: No AWS access currently, implementing features instead
- **Performance Optimization Decision Made**: Vector DB + Redis hybrid architecture (see `docs/DECISIONS.md`)
  - Based on comprehensive latency testing in `docs/OPENAI_LATENCY_TESTING.md`
  - **Problem**: 17s OpenAI latency with gpt-4o-mini (model choice is bottleneck)
  - **Solution**: Pinecone Serverless + OpenAI embeddings + Redis caching
  - **Expected Result**: <1s for vector DB hits, 2-5s for OpenAI fallback (edge cases)
  - **Prerequisites**: Scale synthetic data to 500-1,000 users first (75 users insufficient for vector DB value)
  - **Architecture**: Hybrid retrieval (vector DB with 0.85 similarity threshold, gpt-4o-mini fallback)
  - **Cost**: 80-90% reduction in OpenAI API costs, mostly free tier (Pinecone 1M vectors, ElastiCache Serverless)
- **Future Enhancement**: Enhance synthetic data generation to include more variance for all persona types

### Technical Decisions Made
- **Node version** - Node.js 20 LTS (documented in techContext.md and .cursor/rules/)
- **Python version** - Python 3.11.9 (upgraded from 3.9.6, venv recreated)
- **Package management** - Consolidated root .gitignore (Python + Node patterns)
- **Shadcn/ui** - Using `shadcn` (not deprecated `shadcn-ui`)
- **Synthetic data format** - JSON files that can seed both SQLite and production databases
- **Random seed** - Set to 42 for reproducibility
- **Data generation patterns** - Persona patterns distributed across users for realistic behavioral signals
- **Database** - SQLite for MVP (file: `backend/spendsense.db`)
- **SQLAlchemy** - Using declarative_base() pattern, relationships configured
- **Pydantic** - v2.5.0 with Literal types for enum validation, date parsing validators
- **CORS** - Configured for localhost:5173 (Vite) and localhost:3000 (React)
- **Frontend Routing** - React Router v7 configured with BrowserRouter, all routes set up
- **Path Aliases** - `@src` alias configured in vite.config.js and jsconfig.json for cleaner imports, all components updated to use `@src` consistently
- **Fast Refresh** - Component files export only React components (not constants/variants) for Fast Refresh compatibility
- **Batching** - Transactions processed in batches of 1000 for performance
- **Idempotency** - Duplicate key errors handled gracefully with 409 status
- **Pattern Detection** - Recurring pattern detection with ±5 day tolerance for weekly/monthly/quarterly intervals
- **Feature Detection** - Modular service design, helper functions reusable across signal types
- **Savings Detection** - Handles multiple savings accounts, calculates per-account growth rates and averages them
- **Credit Detection** - Queries credit card accounts and liabilities, calculates utilization metrics, detects payment patterns and overdue status
- **Income Detection** - Detects payroll deposits, calculates pay frequency, income variability, cash flow buffer, and average monthly income
- **Feature Computation** - Combines all signals, saves to database, provides API endpoints for computation and retrieval
- **Frontend Dashboard** - Operator dashboard complete with metrics cards, charts (persona distribution, recommendation status), loading/error states, responsive layout
- **Frontend User List** - Operator user list complete with table, filters, pagination, search, loading/error states, responsive layout
- **Frontend User Detail** - Operator user detail page complete with two-column layout, tabs, signal displays, recommendations section, back navigation, loading/error states
- **Frontend Components** - UserInfoCard, PersonaDisplay, SignalDisplay (all 4 signal types), Progress component
- **Frontend Enums** - Centralized enum system for UserType, ConsentStatus, ConsentAction with helper functions
- **Backend Users Endpoint** - GET /users with pagination, filters, and persona data; GET /users/{user_id} for single user
- **Backend Operator Endpoints** - GET /operator/dashboard with metrics and statistics; GET /operator/users/{user_id}/signals for detailed signals
- **OpenAI Integration** - OpenAI SDK installed (v2.7.1, upgraded from 1.3.5), API key configured, prompt templates created
- **Prompt System** - Self-contained persona-specific prompts following "just right" calibration guide, prompt loader utility with caching
- **Guardrails Service** - Complete guardrails service implemented (PR #20):
  - Tone validation with structured warnings (critical/notable) stored in metadata_json
  - Consent checking, eligibility validation (income, credit, accounts)
  - Partner offer filtering, mandatory disclosure appending
  - ALL recommendations persisted regardless of warnings for operator review
  - Operator UI will display RED (critical) and YELLOW (notable) warnings
  - Operator can approve/decline regardless of warnings

### Integration Points
- Frontend ↔ Backend: CORS configured, API client setup complete (`frontend/src/lib/api.js`), API service functions ready (`frontend/src/lib/apiService.js`), routing structure in place
- Backend ↔ Database: SQLAlchemy setup complete, all models implemented, data loaded (PR #3, #5 complete)
- Backend ↔ Feature Detection: Service module complete with all 4 signal types (PR #6, #7, #8, #9 complete)
- Backend ↔ Feature API: Features and profile endpoints created (PR #10 complete)
- Backend ↔ OpenAI: API key management via environment variables
- Backend ↔ AWS: S3 bucket created (`spendsense-analytics-goico`), parquet exports working
- Data Generation ↔ Database: ✅ Complete - All synthetic data ingested successfully

## Current Blockers
None - Ready to proceed with PR #21 (Recommendation Generation Endpoint)

## Active Questions
1. ✅ Python venv upgraded to 3.11.9 - Complete
2. ✅ All feature detection signals complete - Complete (PR #6-9)
3. ✅ Feature computation endpoint complete - Complete (PR #10)
4. ✅ AWS credentials configured for S3 exports - Complete (PR #29)
5. Should we implement persona assignment before or after frontend setup?

## Workflow Notes
- PR #1 complete (all 41 tasks checked off)
- PR #2 complete (all 38 tasks checked off)
- PR #3 complete (all 58 tasks checked off)
- PR #4 complete (all 31 tasks checked off)
- PR #5 complete (all 30 tasks checked off)
- PR #6 complete (all 22 tasks checked off)
- PR #7 complete (all 20 tasks checked off)
- PR #8 complete (all 21 tasks checked off)
- PR #9 complete (all 33 tasks checked off)
- PR #10 complete (all 28 tasks checked off)
- PR #11 complete (all 39 tasks checked off)
- PR #12 complete (all 39 implementation tasks checked off)
- PR #13 complete (all 61 tasks checked off, including backend endpoints)
- PR #14 complete (all 52 implementation tasks checked off, including backend endpoints)
- PR #15 complete (all 29 tasks checked off - Persona Assignment Service)
- PR #16 complete (all 29 tasks checked off - Persona Assignment Endpoint & Batch Script)
- PR #17 complete (all 50 tasks checked off - OpenAI Integration Setup & Prompt Templates)
- PR #18 complete (all 34 tasks checked off - Recommendation Engine Service - Context Building)
- PR #19 complete (all 26 tasks checked off - Recommendation Engine Service - OpenAI Integration)
- PR #20 complete (all 38 tasks checked off - Guardrails Service - Tone & Consent Validation)
- PR #21 complete (all 44 tasks checked off - Recommendation Generation Endpoint)
- PR #22 complete (all 23 tasks checked off - Get Recommendations Endpoint)
- PR #23 complete (all 26 tasks checked off - Approve Recommendation Endpoint)
- PR #24 complete (all 39 tasks checked off - Override & Reject Endpoints)
- PR #25 complete (all 21 tasks checked off - Bulk Approve Endpoint)
- PR #26 complete (all 51 tasks checked off - Frontend Approval Queue Page)
- PR #27 complete (all 44 tasks checked off - User Dashboard & Consent)
- PR #28 complete (all 54 tasks checked off - Evaluation Script - Metrics Computation)
- PR #29 complete (all 45 tasks checked off - Parquet Export & S3 Integration)
- Following tasks-7.md structure (PR #30 next - Evaluation API Endpoint)
- Synthetic data generation produces JSON files that can be reused as seeds
- Data includes realistic persona patterns for testing feature detection
- All AI recommendations require operator approval before user visibility
- Consent is mandatory - no recommendations without opt-in
- Database schema matches synthetic data structure exactly
- Pydantic schemas validated and ready for API endpoints
- Data ingestion endpoint functional and tested
- All synthetic data successfully loaded into database
- Feature detection service complete with all 4 signal types (subscription, savings, credit, income)
- Feature computation endpoint and batch script working
- 142 feature records computed and saved to database (71 users × 2 windows)

